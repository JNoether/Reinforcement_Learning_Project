{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import torch \n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from MDP import MDP\n",
    "\n",
    "import stable_baselines3\n",
    "import sb3_contrib\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3080'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Neural Network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    input : 2 X 4 X 4 grid\n",
    "    label : Move [0;6]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(11, 32, 2, padding = 1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding= 1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3)\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "\n",
    "        self.out = nn.Linear(64, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "\n",
    "        x = F.relu(self.lstm(x))\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = F.relu(self.conv4(x))\n",
    "\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.out(x)\n",
    "    \n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(11, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=6, bias=True)\n",
      ")\n",
      "number of parameters: 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating model\n",
    "net = Net()\n",
    "net.cuda()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(f\"number of parameters: {len(params)}\")\n",
    "\n",
    "#loss function\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom environment\n",
    "from gym import spaces\n",
    "\n",
    "class Gridworld(gym.Env):\n",
    "\n",
    "    metadata = {\"render.modes\" : [\"human\"]}\n",
    "\n",
    "    def __init__(self, episodes, dir = [\"data_easy\", \"generated_easy\", \"data_medium\", \"generated_med\", \"data\", \"generated_imitation\"], type = \"train\", \n",
    "                p = [0.1, 0.1, 0.15, 0.1, 0.15, 0.4], lambda1 = 0.01, lambda2 = 0.1, lambda3 = 1, load_optimal = False, epsilon = 0.1) -> None:\n",
    "        super(Gridworld, self).__init__()\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.observation_space = spaces.Box(low = 0, high = 1, shape = (11, 4, 4))\n",
    "\n",
    "        #available MDPs\n",
    "        self.dir = dir\n",
    "        self.type = type\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.lambda3 = lambda3\n",
    "\n",
    "        self.actions = [\"move\", \"turnLeft\", \"turnRight\", \"pickMarker\", \"putMarker\",\"finish\"]\n",
    "        self.actionsDict = {\"move\" : 0, \"turnLeft\" : 1, \"turnRight\" : 2, \"pickMarker\" : 3, \"putMarker\" : 4, \"finish\" : 5}\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.load_optimal = load_optimal\n",
    "\n",
    "        #generate curriculum\n",
    "        #{\"data_easy\" : 15% ,\"generated_easy\" : 20% , \"data_medium\" : 15% , \"generated_med\" : 15% \"data\" : 30%, \"generated_imitation\" : 15%}\n",
    "        self.curriculum = []\n",
    "        #data_easy\n",
    "        for directory,amount in zip(dir, p):\n",
    "            tasks = list(np.random.choice(os.listdir(os.sep.join([\"datasets\", directory, type, \"task\"])), int(episodes * amount)))\n",
    "            tasks = zip([directory for _ in range(int(episodes * amount))], tasks)\n",
    "            self.curriculum += tasks\n",
    "        self.currentTask = 0\n",
    "    \n",
    "\n",
    "    def construct_feature_matrices(self, matrix):\n",
    "        \"\"\"\n",
    "        creates the input for the Neural Network: 11 x 4 x 4 boolean tensor for:\n",
    "        - if a wall is present\n",
    "        - if a marker must be picked up\n",
    "        - if a marker must be put down\n",
    "        - all ones if the marker in the current position needs to be picked up\n",
    "        - all ones if a marker in the current position needs to be put down\n",
    "        - if the agent facing left, down, right, up in the pregrid (one for each direction)\n",
    "        - if the agent facing left, down, right, up in the postgrid (one for each direction)\n",
    "        \"\"\"\n",
    "        wallMat = matrix[0] == 10\n",
    "\n",
    "        marker_post = (matrix[0] > 4) & (matrix[0] < 10)\n",
    " \n",
    "        marker_pre = (matrix[1] > 4) & (matrix[1] < 10)\n",
    "\n",
    "        #using a xor operation\n",
    "        marker_pick = marker_post & np.logical_not(marker_pre)\n",
    "        marker_put = marker_pre & np.logical_not(marker_post)\n",
    "\n",
    "        \"\"\"\n",
    "        if marker_pick[self.currentMDP.agentPosition]:\n",
    "            curr_pick_up = np.ones((4,4))\n",
    "        else:\n",
    "            curr_pick_up = np.zeros((4,4))\n",
    "\n",
    "        if marker_put[self.currentMDP.agentPosition]:\n",
    "            curr_put_down = np.ones((4,4))\n",
    "        else:\n",
    "            curr_put_down = np.zeros((4,4))\n",
    "        \"\"\"\n",
    "\n",
    "        agent_pre = np.zeros((4,4,4))\n",
    "        agent_dir = (matrix[(0, *self.currentMDP.agentPosition)] - 1) % 4\n",
    "        agent_pre[(int(agent_dir), *self.currentMDP.agentPosition)] = 1\n",
    "        \n",
    "        agent_post = np.zeros((4,4,4))\n",
    "        agent_pos_post = ((matrix[1] > 0) & (matrix[1] < 9)).nonzero()\n",
    "        agent_dir = (matrix[(1, *agent_pos_post)] - 1) % 4\n",
    "        agent_post[(int(agent_dir), *agent_pos_post)] = 1\n",
    "        \n",
    "        return np.array([wallMat, *agent_pre, *agent_post, marker_pick, marker_put])\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        nextDir, self.nexti = self.curriculum[self.currentTask]\n",
    "        self.nexti = re.sub(r\"\\D\", \"\", self.nexti)\n",
    "        self.currentTask += 1\n",
    "        self.currentMDP = MDP(nextDir, self.type, self.nexti, lambda1= self.lambda1, lambda2 = self.lambda2, lambda3 =self.lambda3)\n",
    "\n",
    "        # load optimal sequence if possible\n",
    "        try:\n",
    "            with open(os.sep.join([\"datasets\", nextDir, self.type, \"seq\", self.nexti + \"_seq.json\"])) as grid:\n",
    "                grid = json.load(grid)\n",
    "                self.optimal_seq = grid[\"sequence\"]\n",
    "                # current step\n",
    "                assert self.optimal_seq[-1] == \"finish\"\n",
    "        except:  \n",
    "            self.optimal_seq = []\n",
    "        self.steps = 0\n",
    "\n",
    "        #with probability 1 - epsilon use either the optimal sequence or better masks\n",
    "        self.use_optimal =  (np.random.rand() < (1 - self.epsilon)) and self.load_optimal\n",
    "\n",
    "        return self.construct_feature_matrices(self.currentMDP.get_current_state())\n",
    "\n",
    "    def step(self, action):\n",
    "        nextState, rew, done, info = self.currentMDP.sample_next_state_and_reward(self.actions[action])\n",
    "        self.steps += 1\n",
    "        if self.steps > 500:\n",
    "            return nextState, -self.lambda3, True, info \n",
    "\n",
    "        return self.construct_feature_matrices(nextState), rew - 0.01, done, info \n",
    "        \n",
    "    def render(self):\n",
    "        self.currentMDP.print_grid()\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def action_masks(self):\n",
    "\n",
    "        mat = self.currentMDP.get_current_state()\n",
    "        if np.array_equal(mat[0], mat[1]):\n",
    "            return np.array([0,0,0,0,0,1])\n",
    "\n",
    "        # force agent to take optimal action if possible\n",
    "        mask = self.currentMDP.action_mask()\n",
    "\n",
    "        if self.use_optimal and self.optimal_seq:\n",
    "            mask = np.zeros(6)\n",
    "            mask[self.actionsDict[self.optimal_seq[self.steps]]] = 1\n",
    "            return mask\n",
    "        \n",
    "        return mask\n",
    "\n",
    "    # functions bellow are only used for inheritance \n",
    "    def get_MDP(self):\n",
    "        return self.currentMDP\n",
    "\n",
    "    def get_MDP_name(self):\n",
    "        return self.nextDir, self.nextType, self.nexti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEnv = Gridworld(100, dir = [\"data\"], load_optimal= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['#' '.' '#' 'v']\n",
      "  ['.' '#' '#' 'O']\n",
      "  ['.' '.' '#' '.']\n",
      "  ['#' '.' '#' '#']]\n",
      "\n",
      " [['#' '.' '#' '.']\n",
      "  ['.' '#' '#' 'O']\n",
      "  ['.' '.' '#' 'r']\n",
      "  ['#' '.' '#' '#']]]\n",
      "[1. 1. 1. 0. 1. 0.]\n",
      "0.0\n",
      "[[['#' '.' '#' '.']\n",
      "  ['.' '#' '#' 'd']\n",
      "  ['.' '.' '#' '.']\n",
      "  ['#' '.' '#' '#']]\n",
      "\n",
      " [['#' '.' '#' '.']\n",
      "  ['.' '#' '#' 'O']\n",
      "  ['.' '.' '#' 'r']\n",
      "  ['#' '.' '#' '#']]]\n",
      "0.0\n",
      "[[['#' '.' '#' '.']\n",
      "  ['.' '#' '#' 'O']\n",
      "  ['.' '.' '#' 'v']\n",
      "  ['#' '.' '#' '#']]\n",
      "\n",
      " [['#' '.' '#' '.']\n",
      "  ['.' '#' '#' 'O']\n",
      "  ['.' '.' '#' 'r']\n",
      "  ['#' '.' '#' '#']]]\n"
     ]
    }
   ],
   "source": [
    "m = testEnv.reset()\n",
    "testEnv.render()\n",
    "print(testEnv.action_masks())\n",
    "m, r, _, _ = testEnv.step(np.argmax(testEnv.action_masks()))\n",
    "print(r)\n",
    "testEnv.render()\n",
    "testEnv.action_masks()\n",
    "_, r, _, _ = testEnv.step(np.argmax(testEnv.action_masks()))\n",
    "print(r)\n",
    "testEnv.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "def test_RL_models(model):\n",
    "    totalCorrect, totalOptimal = 0,0\n",
    "    for dir, num in zip([\"data_easy\", \"data_medium\", \"data\"], [80, 24, 480]):\n",
    "        print(\"current data: \" + dir)\n",
    "        total_steps = 0\n",
    "        valDataset = Gridworld(num * 10, dir = [dir], p = [1], type = \"val\", lambda1=0, lambda2=0, load_optimal= False)\n",
    "        correct, total, optimal = 0, num*10, 0\n",
    "        for task in range(int(total)):\n",
    "            if task % num == num-1:\n",
    "                print(f\"{(task+1) / total *100} %, running acc: {(correct*100)/(task+1)}, task solved optimaly: {optimal*100/task} %, average steps to solve: {total_steps / task}\")\n",
    "            currMDP = valDataset.reset()\n",
    "            lenOptimalSeq = len(valDataset.optimal_seq)\n",
    "            done = False\n",
    "            steps = 0\n",
    "            while not done and steps < 50:\n",
    "                action = model.predict(currMDP, action_masks = valDataset.action_masks(), deterministic = False)[0]\n",
    "                currMDP, rew, done, _ = valDataset.step(action)\n",
    "                steps += 1\n",
    "                if rew > 0:\n",
    "                    correct += 1\n",
    "                    total_steps += steps\n",
    "                    if steps == lenOptimalSeq:\n",
    "                        optimal += 1\n",
    "        totalCorrect += correct\n",
    "        totalOptimal += optimal\n",
    "    print(f\"Total Accuracy : {totalCorrect*100/5840}, Solved Optimally : {totalOptimal *100 / 5840}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class CustomFeatureExtractorTorch(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 1152):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        self.conv1 = net.conv1\n",
    "        self.conv2 = net.conv2\n",
    "        self.conv3 = net.conv3\n",
    "        #self.conv4 = net.conv4\n",
    "\n",
    "        self.fc1 = net.fc1\n",
    "        self.fc2 = net.fc2\n",
    "  \n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.conv1(input))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #x = F.relu(self.conv4(x))\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net_arch = [\n",
    "    256, 128,\n",
    "    dict(vf = [64, 32, 18], pi = [32, 16, 6])\n",
    "]\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class = CustomFeatureExtractorTorch,\n",
    "    net_arch = net_arch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "\n",
    "from stable_baselines3.ppo import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70       |\n",
      "|    ep_rew_mean     | 0.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 1197     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 194          |\n",
      "|    ep_rew_mean          | -1.51        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 797          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 2000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030291067 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | -0.518       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0137      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00335     |\n",
      "|    value_loss           | 0.0217       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 197         |\n",
      "|    ep_rew_mean          | -1.64       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 725         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008149818 |\n",
      "|    clip_fraction        | 0.0554      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00647    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00477    |\n",
      "|    value_loss           | 0.012       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 169         |\n",
      "|    ep_rew_mean          | -1.29       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 694         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006054559 |\n",
      "|    clip_fraction        | 0.0499      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | -0.0491     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00876    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00858    |\n",
      "|    value_loss           | 0.00681     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 183         |\n",
      "|    ep_rew_mean          | -1.49       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 675         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007980647 |\n",
      "|    clip_fraction        | 0.0948      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0151     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00912    |\n",
      "|    value_loss           | 0.00693     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 190         |\n",
      "|    ep_rew_mean          | -1.54       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 662         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011108908 |\n",
      "|    clip_fraction        | 0.0798      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0534     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.00843     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 160          |\n",
      "|    ep_rew_mean          | -1.14        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 653          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 7000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071329013 |\n",
      "|    clip_fraction        | 0.0961       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0.557        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0295      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00909     |\n",
      "|    value_loss           | 0.00883      |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 172        |\n",
      "|    ep_rew_mean          | -1.31      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 646        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 8000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01705468 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.2       |\n",
      "|    explained_variance   | 0.311      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00171   |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    value_loss           | 0.012      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 166         |\n",
      "|    ep_rew_mean          | -1.23       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015481695 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0385     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    value_loss           | 0.0049      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 145         |\n",
      "|    ep_rew_mean          | -0.952      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018008428 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0427     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    value_loss           | 0.011       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | -1.02       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 634         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013007371 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0499     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.009       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 152        |\n",
      "|    ep_rew_mean          | -1.05      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 12000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01528128 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.17      |\n",
      "|    explained_variance   | 0.475      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0591    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.027     |\n",
      "|    value_loss           | 0.00981    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 154         |\n",
      "|    ep_rew_mean          | -1.07       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 630         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 13000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022552725 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.536       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0448     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    value_loss           | 0.006       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 158        |\n",
      "|    ep_rew_mean          | -1.14      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 22         |\n",
      "|    total_timesteps      | 14000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02528632 |\n",
      "|    clip_fraction        | 0.242      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.2       |\n",
      "|    explained_variance   | 0.56       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0545    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0349    |\n",
      "|    value_loss           | 0.0071     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 164         |\n",
      "|    ep_rew_mean          | -1.23       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 627         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024499197 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0782     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.038      |\n",
      "|    value_loss           | 0.00559     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 163         |\n",
      "|    ep_rew_mean          | -1.21       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022872195 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0798     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0366     |\n",
      "|    value_loss           | 0.00564     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 145         |\n",
      "|    ep_rew_mean          | -0.965      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 624         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022546645 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0521     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    value_loss           | 0.0114      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 143        |\n",
      "|    ep_rew_mean          | -0.925     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 623        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 28         |\n",
      "|    total_timesteps      | 18000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02066212 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 0.521      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0568    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0388    |\n",
      "|    value_loss           | 0.0222     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 129        |\n",
      "|    ep_rew_mean          | -0.745     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 621        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 19000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03527703 |\n",
      "|    clip_fraction        | 0.284      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.15      |\n",
      "|    explained_variance   | 0.228      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0387    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0479    |\n",
      "|    value_loss           | 0.0115     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 129         |\n",
      "|    ep_rew_mean          | -0.748      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 621         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034830585 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0483     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | -0.438      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 620         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029445406 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0646     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    value_loss           | 0.0111      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 105         |\n",
      "|    ep_rew_mean          | -0.405      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 619         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035567485 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0454     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 0.0161      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 67.8        |\n",
      "|    ep_rew_mean          | 0.123       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 618         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034688532 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.545       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0467     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 64.1        |\n",
      "|    ep_rew_mean          | 0.18        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 617         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039058242 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0563     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0468     |\n",
      "|    value_loss           | 0.0218      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 61.2        |\n",
      "|    ep_rew_mean          | 0.209       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 616         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035693936 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.35        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0563     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    value_loss           | 0.0145      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 71.6       |\n",
      "|    ep_rew_mean          | 0.0653     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 616        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 42         |\n",
      "|    total_timesteps      | 26000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03995095 |\n",
      "|    clip_fraction        | 0.379      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.405      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0826    |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    value_loss           | 0.014      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 64.5        |\n",
      "|    ep_rew_mean          | 0.156       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 615         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031753127 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0406     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    value_loss           | 0.0141      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 66.2        |\n",
      "|    ep_rew_mean          | 0.139       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 614         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031099988 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.721       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.041      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    value_loss           | 0.017       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 61.6        |\n",
      "|    ep_rew_mean          | 0.204       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 614         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029526882 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0132     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0339     |\n",
      "|    value_loss           | 0.0152      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 71.6        |\n",
      "|    ep_rew_mean          | 0.065       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 614         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039294567 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000318   |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0402     |\n",
      "|    value_loss           | 0.0203      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 79.3       |\n",
      "|    ep_rew_mean          | -0.0515    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 613        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 50         |\n",
      "|    total_timesteps      | 31000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04381052 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.645      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0604    |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0348    |\n",
      "|    value_loss           | 0.0092     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 82.9        |\n",
      "|    ep_rew_mean          | -0.107      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 613         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024956655 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | -0.0913     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0247     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0342     |\n",
      "|    value_loss           | 0.0233      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 67.3        |\n",
      "|    ep_rew_mean          | 0.108       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 612         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027762294 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0676     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    value_loss           | 0.0126      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 62          |\n",
      "|    ep_rew_mean          | 0.181       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 612         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042731635 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.605       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 0.0169      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 67.7        |\n",
      "|    ep_rew_mean          | 0.124       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046774805 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.969      |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0904     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0467     |\n",
      "|    value_loss           | 0.0279      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 73.1        |\n",
      "|    ep_rew_mean          | 0.0697      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 612         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040486466 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0547     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    value_loss           | 0.0184      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 73          |\n",
      "|    ep_rew_mean          | 0.0705      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 612         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039498337 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    value_loss           | 0.0108      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 51.9      |\n",
      "|    ep_rew_mean          | 0.382     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 612       |\n",
      "|    iterations           | 38        |\n",
      "|    time_elapsed         | 62        |\n",
      "|    total_timesteps      | 38000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0445875 |\n",
      "|    clip_fraction        | 0.317     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.99     |\n",
      "|    explained_variance   | 0.64      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0878   |\n",
      "|    n_updates            | 370       |\n",
      "|    policy_gradient_loss | -0.0392   |\n",
      "|    value_loss           | 0.0111    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 56.2        |\n",
      "|    ep_rew_mean          | 0.339       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 612         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059597284 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0556     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 0.0261      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49          |\n",
      "|    ep_rew_mean          | 0.411       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044753954 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.972      |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0837     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0477     |\n",
      "|    value_loss           | 0.0149      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 26.8       |\n",
      "|    ep_rew_mean          | 0.672      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 611        |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 67         |\n",
      "|    total_timesteps      | 41000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03745269 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.929     |\n",
      "|    explained_variance   | 0.681      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0532    |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0495    |\n",
      "|    value_loss           | 0.0165     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20.3        |\n",
      "|    ep_rew_mean          | 0.777       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036297817 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.852      |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.055      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0398     |\n",
      "|    value_loss           | 0.0192      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 22.5        |\n",
      "|    ep_rew_mean          | 0.755       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 43000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031450998 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.895      |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.068      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0492     |\n",
      "|    value_loss           | 0.0156      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 28.9      |\n",
      "|    ep_rew_mean          | 0.691     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 610       |\n",
      "|    iterations           | 44        |\n",
      "|    time_elapsed         | 72        |\n",
      "|    total_timesteps      | 44000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0408246 |\n",
      "|    clip_fraction        | 0.287     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.879    |\n",
      "|    explained_variance   | 0.689     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0511   |\n",
      "|    n_updates            | 430       |\n",
      "|    policy_gradient_loss | -0.0405   |\n",
      "|    value_loss           | 0.0168    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.8        |\n",
      "|    ep_rew_mean          | 0.612       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 609         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050605766 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.884      |\n",
      "|    explained_variance   | 0.478       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0433     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    value_loss           | 0.0154      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 38.5        |\n",
      "|    ep_rew_mean          | 0.536       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 609         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059835874 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0412     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0453     |\n",
      "|    value_loss           | 0.0112      |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "episodes = 5 * 1e5\n",
    " \n",
    "FinalEnv = make_vec_env(Gridworld, n_envs= 2  , \n",
    "    env_kwargs={\"episodes\" : episodes, \"lambda1\" : 0, \"lambda2\" : 0, \"lambda3\": 1, \n",
    "            \"dir\" : [\"data_easy\", \"data_medium\", \"data\"], \"p\" : [0.3, 0.2, 0.5],\n",
    "             \"load_optimal\" : True, \"epsilon\" : 0.5})\n",
    "\n",
    "FinalModel = MaskablePPO(MaskableActorCriticPolicy, FinalEnv,  policy_kwargs = policy_kwargs, verbose = 1, n_steps= 500)#\n",
    "\n",
    "FinalModel.learn(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current data: data_easy\n",
      "10.0 %, running acc: 98.75, task solved optimaly: 97.46835443037975 %, average steps to solve: 2.607594936708861\n",
      "20.0 %, running acc: 99.375, task solved optimaly: 97.48427672955975 %, average steps to solve: 2.6163522012578615\n",
      "30.0 %, running acc: 99.58333333333333, task solved optimaly: 97.07112970711297 %, average steps to solve: 2.6610878661087867\n",
      "40.0 %, running acc: 99.6875, task solved optimaly: 96.23824451410658 %, average steps to solve: 2.670846394984326\n",
      "50.0 %, running acc: 99.75, task solved optimaly: 95.73934837092732 %, average steps to solve: 2.6791979949874687\n",
      "60.0 %, running acc: 99.79166666666667, task solved optimaly: 95.82463465553236 %, average steps to solve: 2.6784968684759916\n",
      "70.0 %, running acc: 99.82142857142857, task solved optimaly: 96.06440071556351 %, average steps to solve: 2.663685152057245\n",
      "80.0 %, running acc: 99.84375, task solved optimaly: 96.40062597809077 %, average steps to solve: 2.647887323943662\n",
      "90.0 %, running acc: 99.86111111111111, task solved optimaly: 96.66203059805285 %, average steps to solve: 2.631432545201669\n",
      "100.0 %, running acc: 99.875, task solved optimaly: 96.87108886107634 %, average steps to solve: 2.6082603254067585\n",
      "current data: data_medium\n",
      "10.0 %, running acc: 58.333333333333336, task solved optimaly: 47.82608695652174 %, average steps to solve: 2.869565217391304\n",
      "20.0 %, running acc: 70.83333333333333, task solved optimaly: 55.319148936170215 %, average steps to solve: 3.404255319148936\n",
      "30.0 %, running acc: 65.27777777777777, task solved optimaly: 53.521126760563384 %, average steps to solve: 2.887323943661972\n",
      "40.0 %, running acc: 65.625, task solved optimaly: 51.578947368421055 %, average steps to solve: 3.4105263157894736\n",
      "50.0 %, running acc: 65.0, task solved optimaly: 52.10084033613445 %, average steps to solve: 3.361344537815126\n",
      "60.0 %, running acc: 66.66666666666667, task solved optimaly: 53.84615384615385 %, average steps to solve: 3.370629370629371\n",
      "70.0 %, running acc: 66.07142857142857, task solved optimaly: 52.69461077844311 %, average steps to solve: 3.467065868263473\n",
      "80.0 %, running acc: 66.14583333333333, task solved optimaly: 52.35602094240838 %, average steps to solve: 3.774869109947644\n",
      "90.0 %, running acc: 68.05555555555556, task solved optimaly: 53.95348837209303 %, average steps to solve: 3.7534883720930234\n",
      "100.0 %, running acc: 69.16666666666667, task solved optimaly: 54.39330543933055 %, average steps to solve: 3.774058577405858\n",
      "current data: data\n",
      "10.0 %, running acc: 50.833333333333336, task solved optimaly: 32.150313152400834 %, average steps to solve: 4.697286012526096\n",
      "20.0 %, running acc: 51.979166666666664, task solved optimaly: 33.472367049009385 %, average steps to solve: 4.762252346193952\n",
      "30.0 %, running acc: 50.833333333333336, task solved optimaly: 32.59207783182766 %, average steps to solve: 4.469770674079221\n",
      "40.0 %, running acc: 50.9375, task solved optimaly: 32.67326732673267 %, average steps to solve: 4.484627410109432\n",
      "50.0 %, running acc: 50.75, task solved optimaly: 32.388495206335975 %, average steps to solve: 4.431846602751146\n",
      "60.0 %, running acc: 50.451388888888886, task solved optimaly: 32.302882945467175 %, average steps to solve: 4.323723515109413\n",
      "70.0 %, running acc: 50.595238095238095, task solved optimaly: 32.30128014289967 %, average steps to solve: 4.369752902649598\n",
      "80.0 %, running acc: 50.338541666666664, task solved optimaly: 32.143787444647046 %, average steps to solve: 4.328210471476947\n",
      "90.0 %, running acc: 50.50925925925926, task solved optimaly: 32.345450335725864 %, average steps to solve: 4.319055336883538\n",
      "100.0 %, running acc: 50.541666666666664, task solved optimaly: 32.485934569702025 %, average steps to solve: 4.291310689727027\n",
      "Total Accuracy : 58.11643835616438, Solved Optimally : 42.20890410958904\n"
     ]
    }
   ],
   "source": [
    "test_RL_models(FinalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current data: data_easy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error: Unexpected observation shape (13, 4, 4) for Box environment, please use (11, 4, 4) or (n_env, 11, 4, 4) for the observation shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20607/3572030753.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskablePPO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RLModel-onlyForbiddenActionMasks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_RL_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_20607/2514005825.py\u001b[0m in \u001b[0;36mtest_RL_models\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrMDP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mcurrMDP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sb3_contrib/ppo_mask/ppo_mask.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, mask, deterministic, action_masks)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecurrent\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \"\"\"\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sb3_contrib/common/maskable/policies.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, mask, deterministic, action_masks)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_training_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mobs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;31m# Dict obs need to be handled separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mvectorized_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_vectorized_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0;31m# Add batch dimension if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/utils.py\u001b[0m in \u001b[0;36mis_vectorized_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mspace_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_vec_obs_func\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mis_vec_obs_func_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_vec_obs_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# for-else happens if no break is called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stable_baselines3/common/utils.py\u001b[0m in \u001b[0;36mis_vectorized_box_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0;34mf\"Error: Unexpected observation shape {observation.shape} for \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;34m+\u001b[0m \u001b[0;34mf\"Box environment, please use {observation_space.shape} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Unexpected observation shape (13, 4, 4) for Box environment, please use (11, 4, 4) or (n_env, 11, 4, 4) for the observation shape."
     ]
    }
   ],
   "source": [
    "comp = MaskablePPO.load(\"RLModel-onlyForbiddenActionMasks\")\n",
    "\n",
    "test_RL_models(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalModel = MaskablePPO.load(\"RLModel-restrictiveActionMasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalModel.save(\"RLModel-onlyForbiddenActionMasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEnv = Gridworld(100,load_optimal= False, dir = [\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['#' '.' '<' '.']\n",
      "  ['.' '.' 'O' '.']\n",
      "  ['.' '.' '.' '.']\n",
      "  ['.' '#' '.' '.']]\n",
      "\n",
      " [['#' '.' '.' '.']\n",
      "  ['.' '.' '.' '>']\n",
      "  ['.' '.' '.' '.']\n",
      "  ['.' '#' '.' '.']]]\n"
     ]
    }
   ],
   "source": [
    "curr = testEnv.reset()\n",
    "testEnv.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 0. 1. 0.]\n",
      "[[['#' '.' '<' '.']\n",
      "  ['.' '.' 'O' '.']\n",
      "  ['.' '.' '.' '.']\n",
      "  ['.' '#' '.' '.']]\n",
      "\n",
      " [['#' '.' '.' '.']\n",
      "  ['.' '.' '.' '>']\n",
      "  ['.' '.' '.' '.']\n",
      "  ['.' '#' '.' '.']]]\n"
     ]
    }
   ],
   "source": [
    "a = FinalModel.predict(curr, action_masks= testEnv.action_masks(), deterministic= False)[0]\n",
    "curr, _, done, _ = testEnv.step(a)\n",
    "print(testEnv.action_masks())\n",
    "testEnv.render()\n",
    "if done:\n",
    "    print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
