{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import torch \n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from MDP import MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3080'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['v' '.' '.' '#']\n",
      "  ['.' '#' '#' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['#' '#' '#' '#']]\n",
      "\n",
      " [['.' '.' '.' '#']\n",
      "  ['O' '#' '#' '#']\n",
      "  ['v' '#' '.' '#']\n",
      "  ['#' '#' '#' '#']]]\n",
      "[[['.' '.' '.' '#']\n",
      "  ['<' '#' '#' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['#' '#' '#' '#']]\n",
      "\n",
      " [['.' '.' '.' '#']\n",
      "  ['O' '#' '#' '#']\n",
      "  ['v' '#' '.' '#']\n",
      "  ['#' '#' '#' '#']]]\n"
     ]
    }
   ],
   "source": [
    "#load MDP\n",
    "mdp = MDP(dir = \"data_medium\", type = \"val\", name = \"100595\")\n",
    "# show grid\n",
    "mdp.print_grid()\n",
    "for a in [\"move\", \"turnRight\", \"pickMarker\", \"move\"]:\n",
    "    mdp.get_next_state(a)\n",
    "mdp.print_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model with Imitation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seq', 'task']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data tests\n",
    "os.listdir(\"datasets/data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "\n",
    "#numerical representation of actions\n",
    "getNumAction = {\n",
    "    \"move\" : 0,\n",
    "    \"turnRight\": 1,\n",
    "    \"turnLeft\" : 2,\n",
    "    \"pickMarker\": 3,\n",
    "    \"putMarker\" : 4,\n",
    "    \"finish\" : 5\n",
    "}\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    attributes:\n",
    "    dir : str list :=  accepted directories (data, data_easy, data_medium)\n",
    "    type : str list := train and/or val\n",
    "    grid : tensor := a tensor of all available grids\n",
    "    actions : tensor := vector of the optimal action for each\n",
    "    \"\"\"\n",
    "    def data_generator(self):\n",
    "        for dir in self.dir:\n",
    "            for type in self.type:\n",
    "                for i in os.listdir(os.sep.join([\"datasets\", dir, type, \"task\"]))[:-4]:\n",
    "                    i = re.sub(r\"\\D\", \"\", i)\n",
    "                    # load MDP and optimal sequence\n",
    "                    currMDP = MDP(dir = dir, type = type, name = str(i))\n",
    "\n",
    "                    with open(os.sep.join([\"datasets\", dir, type, \"seq\", str(i) + \"_seq.json\"])) as seq:\n",
    "                        sequence = json.load(seq)[\"sequence\"]\n",
    "                    \n",
    "                    for action in sequence:\n",
    "                        yield currMDP.get_current_state().copy(), action\n",
    "                        currMDP.get_next_state(action)\n",
    "\n",
    "\n",
    "    def __init__(self, dir = [\"data\", \"data_easy\", \"data_medium\"], type = [\"train\"]) -> None:\n",
    "        \"\"\"\n",
    "        dir : str list :=  accepted directories (data, data_easy, data_medium)\n",
    "        type : str list := train and/or val\n",
    "        \"\"\"\n",
    "        self.dir = dir\n",
    "        self.type = type\n",
    "        lstActionsAndGrids = list(self.data_generator())\n",
    "        self.grid = torch.tensor(np.array([x[0] for x in lstActionsAndGrids]) / 10)\n",
    "        self.actions = torch.tensor(np.array([getNumAction[x[1]] for x in lstActionsAndGrids]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.grid)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.grid[idx], self.actions[idx]\n",
    "\n",
    "trainDataset = Dataset()\n",
    "valDataset = Dataset(type = [\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [1.0000, 0.0000, 1.0000, 0.0000],\n",
      "         [0.0000, 0.4000, 0.9000, 0.0000],\n",
      "         [0.0000, 0.0000, 1.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [1.0000, 0.4000, 1.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.9000, 0.0000],\n",
      "         [0.0000, 0.0000, 1.0000, 0.0000]]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.4000, 0.0000],\n",
       "        [0.0000, 1.0000, 0.9000, 1.0000],\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testGrid = trainDataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Neural Network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    input : 2 X 4 X 4 grid\n",
    "    label : Move [0;6]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # first layer: input\n",
    "        self.conv1 = nn.Conv2d(2, 8, 2)\n",
    "\n",
    "        #second layer : 2nd convolution\n",
    "        self.conv2 = nn.Conv2d(8, 16, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 32, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(32, 16)\n",
    "\n",
    "        self.out = nn.Linear(16, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.out(x)\n",
    "    \n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(2, 8, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (out): Linear(in_features=16, out_features=6, bias=True)\n",
      ")\n",
      "number of parameters: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating model\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(f\"number of parameters: {len(params)}\")\n",
    "\n",
    "#loss function\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "dataloader = data.DataLoader(trainDataset, BATCH_SIZE)\n",
    "validationLoader = data.DataLoader(valDataset, BATCH_SIZE)\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        lossVal = loss(outputs, labels)\n",
    "        lossVal.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += lossVal.item()\n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "  batch 1000 loss: 1.5842277437448502\n",
      "  batch 2000 loss: 1.5768979654312134\n",
      "  batch 3000 loss: 1.5450552382469178\n",
      "  batch 4000 loss: 1.3469123649001122\n",
      "  batch 5000 loss: 1.2608326620459556\n",
      "  batch 6000 loss: 1.225639100253582\n",
      "  batch 7000 loss: 1.2060368137359618\n",
      "  batch 8000 loss: 0.928632303237915\n",
      "  batch 9000 loss: 1.1530471400022506\n",
      "  batch 10000 loss: 1.1261671338677406\n",
      "Loss train 1.1261671338677406 validation 1.104140043258667\n",
      "EPOCH 2\n",
      "  batch 1000 loss: 1.100673337340355\n",
      "  batch 2000 loss: 1.0736198697686195\n",
      "  batch 3000 loss: 1.0347667849063873\n",
      "  batch 4000 loss: 1.0116595275700093\n",
      "  batch 5000 loss: 0.9962882779240608\n",
      "  batch 6000 loss: 0.9725821277499199\n",
      "  batch 7000 loss: 0.9538533962965011\n",
      "  batch 8000 loss: 0.7226288927346468\n",
      "  batch 9000 loss: 0.8231359447538853\n",
      "  batch 10000 loss: 0.7964871341437101\n",
      "Loss train 0.7964871341437101 validation 0.8486955165863037\n",
      "EPOCH 3\n",
      "  batch 1000 loss: 0.9036184303462506\n",
      "  batch 2000 loss: 0.8607337115108967\n",
      "  batch 3000 loss: 0.8514104889035224\n",
      "  batch 4000 loss: 0.8422077465057373\n",
      "  batch 5000 loss: 0.835196524322033\n",
      "  batch 6000 loss: 0.8412309179604054\n",
      "  batch 7000 loss: 0.8180760237574577\n",
      "  batch 8000 loss: 0.6141362084150315\n",
      "  batch 9000 loss: 0.6315158043503761\n",
      "  batch 10000 loss: 0.6117281415984035\n",
      "Loss train 0.6117281415984035 validation 0.7764946222305298\n",
      "EPOCH 4\n",
      "  batch 1000 loss: 0.8172767052054405\n",
      "  batch 2000 loss: 0.7920490810573101\n",
      "  batch 3000 loss: 0.7903968388140201\n",
      "  batch 4000 loss: 0.787494524359703\n",
      "  batch 5000 loss: 0.7807282779216766\n",
      "  batch 6000 loss: 0.7652723288536072\n",
      "  batch 7000 loss: 0.7544524990916253\n",
      "  batch 8000 loss: 0.5759886377975345\n",
      "  batch 9000 loss: 0.5606198268011212\n",
      "  batch 10000 loss: 0.5681609690412879\n",
      "Loss train 0.5681609690412879 validation 0.7583949565887451\n",
      "EPOCH 5\n",
      "  batch 1000 loss: 0.7919258132278919\n",
      "  batch 2000 loss: 0.755847628235817\n",
      "  batch 3000 loss: 0.7575196372568608\n",
      "  batch 4000 loss: 0.7435784175544977\n",
      "  batch 5000 loss: 0.7365835667550564\n",
      "  batch 6000 loss: 0.736394257068634\n",
      "  batch 7000 loss: 0.7510163429379463\n",
      "  batch 8000 loss: 0.531280385531485\n",
      "  batch 9000 loss: 0.5237611504159868\n",
      "  batch 10000 loss: 0.5391497599631548\n",
      "Loss train 0.5391497599631548 validation 0.7273151874542236\n",
      "EPOCH 6\n",
      "  batch 1000 loss: 0.7552978743314743\n",
      "  batch 2000 loss: 0.7199799351394176\n",
      "  batch 3000 loss: 0.722157592266798\n",
      "  batch 4000 loss: 0.7270796581208706\n",
      "  batch 5000 loss: 0.7081104905456305\n",
      "  batch 6000 loss: 0.7038276090323925\n",
      "  batch 7000 loss: 0.7135036500096321\n",
      "  batch 8000 loss: 0.4968628298155963\n",
      "  batch 9000 loss: 0.5016630261726678\n",
      "  batch 10000 loss: 0.48446826910600066\n",
      "Loss train 0.48446826910600066 validation 0.6800292730331421\n",
      "EPOCH 7\n",
      "  batch 1000 loss: 0.7365197842568159\n",
      "  batch 2000 loss: 0.695625560849905\n",
      "  batch 3000 loss: 0.6989621668756009\n",
      "  batch 4000 loss: 0.692275537982583\n",
      "  batch 5000 loss: 0.6915158339887857\n",
      "  batch 6000 loss: 0.6848805628269911\n",
      "  batch 7000 loss: 0.6771514530479908\n",
      "  batch 8000 loss: 0.49255783135816456\n",
      "  batch 9000 loss: 0.4735253702774644\n",
      "  batch 10000 loss: 0.4511893253289163\n",
      "Loss train 0.4511893253289163 validation 0.6718418002128601\n",
      "EPOCH 8\n",
      "  batch 1000 loss: 0.7214790805876256\n",
      "  batch 2000 loss: 0.6733226961642503\n",
      "  batch 3000 loss: 0.671435842037201\n",
      "  batch 4000 loss: 0.6875719951093197\n",
      "  batch 5000 loss: 0.6609598950147629\n",
      "  batch 6000 loss: 0.6866137419939041\n",
      "  batch 7000 loss: 0.660976793512702\n",
      "  batch 8000 loss: 0.47755813747271897\n",
      "  batch 9000 loss: 0.44243036799877883\n",
      "  batch 10000 loss: 0.4442793928869069\n",
      "Loss train 0.4442793928869069 validation 0.6645534038543701\n",
      "EPOCH 9\n",
      "  batch 1000 loss: 0.7002067732810974\n",
      "  batch 2000 loss: 0.6605246375799179\n",
      "  batch 3000 loss: 0.678081832692027\n",
      "  batch 4000 loss: 0.6577453160136938\n",
      "  batch 5000 loss: 0.6550834648013115\n",
      "  batch 6000 loss: 0.6497438428997994\n",
      "  batch 7000 loss: 0.661716634824872\n",
      "  batch 8000 loss: 0.4422711180560291\n",
      "  batch 9000 loss: 0.4409220895729959\n",
      "  batch 10000 loss: 0.416031111124903\n",
      "Loss train 0.416031111124903 validation 0.6628751754760742\n",
      "EPOCH 10\n",
      "  batch 1000 loss: 0.7006517150253058\n",
      "  batch 2000 loss: 0.6449964769929647\n",
      "  batch 3000 loss: 0.6468954557329416\n",
      "  batch 4000 loss: 0.6445705355256796\n",
      "  batch 5000 loss: 0.6363700440078974\n",
      "  batch 6000 loss: 0.6380305107831955\n",
      "  batch 7000 loss: 0.6476301658004522\n",
      "  batch 8000 loss: 0.4423871459364891\n",
      "  batch 9000 loss: 0.42216386706754566\n",
      "  batch 10000 loss: 0.4069934692122042\n",
      "Loss train 0.4069934692122042 validation 0.6337599754333496\n",
      "EPOCH 11\n",
      "  batch 1000 loss: 0.6871957062780857\n",
      "  batch 2000 loss: 0.6336096842288971\n",
      "  batch 3000 loss: 0.6339168202131986\n",
      "  batch 4000 loss: 0.6585069626420736\n",
      "  batch 5000 loss: 0.6480041069239378\n",
      "  batch 6000 loss: 0.6191421571075917\n",
      "  batch 7000 loss: 0.6334474870115518\n",
      "  batch 8000 loss: 0.41481148841418325\n",
      "  batch 9000 loss: 0.41149394574016335\n",
      "  batch 10000 loss: 0.44108361376822\n",
      "Loss train 0.44108361376822 validation 0.6179815530776978\n",
      "EPOCH 12\n",
      "  batch 1000 loss: 0.6749433851987123\n",
      "  batch 2000 loss: 0.6182982887774706\n",
      "  batch 3000 loss: 0.6154508670568466\n",
      "  batch 4000 loss: 0.6251715820431709\n",
      "  batch 5000 loss: 0.6328750176280737\n",
      "  batch 6000 loss: 0.6048853416591883\n",
      "  batch 7000 loss: 0.6265379393398762\n",
      "  batch 8000 loss: 0.40528259600885214\n",
      "  batch 9000 loss: 0.3959922391604632\n",
      "  batch 10000 loss: 0.3882273152396083\n",
      "Loss train 0.3882273152396083 validation 0.6240448951721191\n",
      "EPOCH 13\n",
      "  batch 1000 loss: 0.6573303494751453\n",
      "  batch 2000 loss: 0.6144906271994114\n",
      "  batch 3000 loss: 0.6124933376163244\n",
      "  batch 4000 loss: 0.6097432963699102\n",
      "  batch 5000 loss: 0.6051057173907757\n",
      "  batch 6000 loss: 0.6037522369772196\n",
      "  batch 7000 loss: 0.6034783710837364\n",
      "  batch 8000 loss: 0.4038956938870251\n",
      "  batch 9000 loss: 0.39092401579394936\n",
      "  batch 10000 loss: 0.3983156634606421\n",
      "Loss train 0.3983156634606421 validation 0.6061971783638\n",
      "EPOCH 14\n",
      "  batch 1000 loss: 0.6463774098157883\n",
      "  batch 2000 loss: 0.6138305309563875\n",
      "  batch 3000 loss: 0.5962557084336877\n",
      "  batch 4000 loss: 0.6057201064378023\n",
      "  batch 5000 loss: 0.6024170668572187\n",
      "  batch 6000 loss: 0.5901334988325835\n",
      "  batch 7000 loss: 0.6139650948867201\n",
      "  batch 8000 loss: 0.40096447318419814\n",
      "  batch 9000 loss: 0.382385159380734\n",
      "  batch 10000 loss: 0.3744646640755236\n",
      "Loss train 0.3744646640755236 validation 0.5934529304504395\n",
      "EPOCH 15\n",
      "  batch 1000 loss: 0.6255122065767645\n",
      "  batch 2000 loss: 0.6348801144361496\n",
      "  batch 3000 loss: 0.5867211994454264\n",
      "  batch 4000 loss: 0.5905377384647728\n",
      "  batch 5000 loss: 0.5975945699214935\n",
      "  batch 6000 loss: 0.582501309543848\n",
      "  batch 7000 loss: 0.5972183695584535\n",
      "  batch 8000 loss: 0.3897643962390721\n",
      "  batch 9000 loss: 0.36689855986461045\n",
      "  batch 10000 loss: 0.3659489874690771\n",
      "Loss train 0.3659489874690771 validation 0.5860414505004883\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "writer = SummaryWriter(\"runs/imitation_learning_{}\".format(timestamp))\n",
    "epoch_num = 0\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH {}\".format(epoch_num + 1))\n",
    "\n",
    "    net.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_num, writer)\n",
    "\n",
    "    net.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validationLoader):\n",
    "        vinputs, vlabels = vdata\n",
    "        vout = net(vinputs)\n",
    "        vloss = loss(vout, vlabels)\n",
    "        running_vloss += vloss\n",
    "    avg_vloss = running_vloss / (i+1)\n",
    "    print(f\"Loss train {avg_loss} validation {avg_vloss}\")\n",
    "\n",
    "\n",
    "    writer.add_scalars(\"Training Loss\", {\"Training\" : avg_loss, \"validation\" : avg_vloss})\n",
    "    writer.flush()\n",
    "\n",
    "    epoch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['>' '.' '.' 'O']\n",
      "  ['#' '#' '.' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['.' '#' '#' '#']]\n",
      "\n",
      " [['.' '.' '.' '^']\n",
      "  ['#' '#' '.' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['.' '#' '#' '#']]]\n",
      "action: move, reward: 0.0\n",
      "[[['.' '>' '.' 'O']\n",
      "  ['#' '#' '.' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['.' '#' '#' '#']]\n",
      "\n",
      " [['.' '.' '.' '^']\n",
      "  ['#' '#' '.' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['.' '#' '#' '#']]]\n",
      "action: move, reward: 0.0\n",
      "[[['.' '.' '>' 'O']\n",
      "  ['#' '#' '.' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['.' '#' '#' '#']]\n",
      "\n",
      " [['.' '.' '.' '^']\n",
      "  ['#' '#' '.' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['.' '#' '#' '#']]]\n",
      "action: move, reward: 0.0\n",
      "[[['.' '.' '.' 'r']\n",
      "  ['#' '#' '.' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['.' '#' '#' '#']]\n",
      "\n",
      " [['.' '.' '.' '^']\n",
      "  ['#' '#' '.' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['.' '#' '#' '#']]]\n",
      "action: turnLeft, reward: 0.0\n",
      "[[['.' '.' '.' 'u']\n",
      "  ['#' '#' '.' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['.' '#' '#' '#']]\n",
      "\n",
      " [['.' '.' '.' '^']\n",
      "  ['#' '#' '.' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['.' '#' '#' '#']]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.625, 5, False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model\n",
    "actions = [\"move\", \"turnRight\", \"turnLeft\", \"pickMarker\", \"putMarker\", \"finish\"]\n",
    "\n",
    "testMDP = MDP(dir = \"data\", type = \"val\", name = \"100112\")\n",
    "\n",
    "def apply_to_grid(MDP, show_grid):\n",
    "    reward = 0\n",
    "    steps = 0\n",
    "    if show_grid:\n",
    "        MDP.print_grid()\n",
    "    while True:\n",
    "        current_grid = torch.tensor(MDP.get_current_state()) / 10\n",
    "        out = net(current_grid.unsqueeze(0))\n",
    "        nextAction = actions[torch.argmax(out)]\n",
    "        reward += MDP.gamma**steps * MDP.reward(nextAction)\n",
    "        steps += 1\n",
    "        if MDP.get_next_state(nextAction) == \"Terminal\" or steps > 100:\n",
    "            return reward, steps, MDP.task_solved()\n",
    "        if show_grid:\n",
    "            print(\"action: {}, reward: {}\".format(nextAction, reward))\n",
    "            MDP.print_grid()\n",
    "\n",
    "\n",
    "apply_to_grid(testMDP, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasks solved with only imitation learning:\n",
      "Total : 2246, accuracy: 0.13035403366221707\n"
     ]
    }
   ],
   "source": [
    "# check accuracy of solved tasks\n",
    "sucesses = 0\n",
    "for dir in [\"data\", \"data_easy\", \"data_medium\"]:\n",
    "    for type in [\"val\"]:\n",
    "        for i in os.listdir(os.sep.join([\"datasets\", dir, type, \"task\"]))[:-4]:\n",
    "            i = re.sub(r\"\\D\", \"\", i)\n",
    "            curr_MDP = MDP(dir, type, i)\n",
    "            if apply_to_grid(curr_MDP, False)[-1]:\n",
    "                sucesses += 1\n",
    "\n",
    "print(\"tasks solved with only imitation learning:\")\n",
    "print(f\"Total : {sucesses}, accuracy: {sucesses / len(valDataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is clearly seen here, imitation learning with the limited amount of training data is not able to solve most of the provided tasks. Therefore, in the following I will use the described PPO approach and after that will create the final model that uses imitation learning to initiate the model and then trains it with maskless PPO to achieve the best possible performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
