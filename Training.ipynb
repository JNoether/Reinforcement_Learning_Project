{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import torch \n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from MDP import MDP\n",
    "\n",
    "import stable_baselines3\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MDP\n",
    "mdp = MDP(dir = \"data_medium\", type = \"val\", name = \"100595\")\n",
    "# show grid\n",
    "done = False\n",
    "for a in [\"move\", \"putMarker\", \"move\"]:\n",
    "    if done:\n",
    "        break\n",
    "    nextState, rew, done, b = mdp.sample_next_state_and_reward(a)\n",
    "    mdp.print_grid()\n",
    "    print(rew)\n",
    "mdp.print_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model with Imitation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data tests\n",
    "os.listdir(\"datasets/data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "\n",
    "#numerical representation of actions\n",
    "getNumAction = {\n",
    "    \"move\" : 0,\n",
    "    \"turnRight\": 1,\n",
    "    \"turnLeft\" : 2,\n",
    "    \"pickMarker\": 3,\n",
    "    \"putMarker\" : 4,\n",
    "    \"finish\" : 5\n",
    "}\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    attributes:\n",
    "    dir : str list :=  accepted directories (data, data_easy, data_medium)\n",
    "    type : str list := train and/or val\n",
    "    grid : tensor := a tensor of all available grids\n",
    "    actions : tensor := vector of the optimal action for each\n",
    "    \"\"\"\n",
    "    def data_generator(self):\n",
    "        for dir in self.dir:\n",
    "            for type in self.type:\n",
    "                for i in os.listdir(os.sep.join([\"datasets\", dir, type, \"task\"]))[:-4]:\n",
    "                    i = re.sub(r\"\\D\", \"\", i)\n",
    "                    # load MDP and optimal sequence\n",
    "                    currMDP = MDP(dir = dir, type = type, name = str(i))\n",
    "\n",
    "                    with open(os.sep.join([\"datasets\", dir, type, \"seq\", str(i) + \"_seq.json\"])) as seq:\n",
    "                        sequence = json.load(seq)[\"sequence\"]\n",
    "                    \n",
    "                    for action in sequence:\n",
    "                        yield currMDP.get_current_state().copy(), action\n",
    "                        currMDP.get_next_state(action)\n",
    "\n",
    "\n",
    "    def __init__(self, dir = [\"data\", \"data_easy\", \"data_medium\"], type = [\"train\"]) -> None:\n",
    "        \"\"\"\n",
    "        dir : str list :=  accepted directories (data, data_easy, data_medium)\n",
    "        type : str list := train and/or val\n",
    "        \"\"\"\n",
    "        self.dir = dir\n",
    "        self.type = type\n",
    "        lstActionsAndGrids = list(self.data_generator())\n",
    "        self.grid = torch.tensor(np.array([x[0] for x in lstActionsAndGrids]) / 10, device= device)\n",
    "        self.actions = torch.tensor(np.array([getNumAction[x[1]] for x in lstActionsAndGrids]), device= device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.grid)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.grid[idx], self.actions[idx]\n",
    "\n",
    "trainDataset = Dataset()\n",
    "valDataset = Dataset(type = [\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Neural Network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    input : 2 X 4 X 4 grid\n",
    "    label : Move [0;6]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # first layer: input\n",
    "        self.conv1 = nn.Conv2d(2, 8, 2)\n",
    "\n",
    "        #second layer : 2nd convolution\n",
    "        self.conv2 = nn.Conv2d(8, 16, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 32, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(32, 16)\n",
    "\n",
    "        self.out = nn.Linear(16, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.out(x)\n",
    "    \n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating model\n",
    "net = Net()\n",
    "net.cuda()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(f\"number of parameters: {len(params)}\")\n",
    "\n",
    "#loss function\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "dataloader = data.DataLoader(trainDataset, BATCH_SIZE)\n",
    "validationLoader = data.DataLoader(valDataset, BATCH_SIZE)\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        lossVal = loss(outputs, labels)\n",
    "        lossVal.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += lossVal.item()\n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "writer = SummaryWriter(\"runs/imitation_learning_{}\".format(timestamp))\n",
    "epoch_num = 0\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "EPOCHS = 14\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH {}\".format(epoch_num + 1))\n",
    "\n",
    "    net.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_num, writer)\n",
    "\n",
    "    net.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validationLoader):\n",
    "        vinputs, vlabels = vdata\n",
    "        vout = net(vinputs)\n",
    "        vloss = loss(vout, vlabels)\n",
    "        running_vloss += vloss\n",
    "    avg_vloss = running_vloss / (i+1)\n",
    "    print(f\"Loss train {avg_loss} validation {avg_vloss}\")\n",
    "\n",
    "\n",
    "    writer.add_scalars(\"Training Loss\", {\"Training\" : avg_loss, \"validation\" : avg_vloss})\n",
    "    writer.flush()\n",
    "\n",
    "    epoch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "actions = [\"move\", \"turnRight\", \"turnLeft\", \"pickMarker\", \"putMarker\", \"finish\"]\n",
    "\n",
    "testMDP = MDP(dir = \"data\", type = \"val\", name = \"100112\")\n",
    "\n",
    "def apply_to_grid(MDP, show_grid):\n",
    "    reward = 0\n",
    "    steps = 0\n",
    "    if show_grid:\n",
    "        MDP.print_grid()\n",
    "    while True:\n",
    "        current_grid = torch.tensor(MDP.get_current_state(), device= device) / 10\n",
    "        out = net(current_grid.unsqueeze(0))\n",
    "        nextAction = actions[torch.argmax(out)]\n",
    "        reward += MDP.gamma**steps * MDP.reward(nextAction)\n",
    "        steps += 1\n",
    "        if MDP.get_next_state(nextAction) == \"Terminal\" or steps > 100:\n",
    "            return reward, steps, MDP.task_solved()\n",
    "        if show_grid:\n",
    "            print(\"action: {}, reward: {}\".format(nextAction, reward))\n",
    "            MDP.print_grid()\n",
    "\n",
    "\n",
    "apply_to_grid(testMDP, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check accuracy of solved tasks\n",
    "sucesses = 0\n",
    "for dir in [\"data\", \"data_easy\", \"data_medium\"]:\n",
    "    for type in [\"val\"]:\n",
    "        for i in os.listdir(os.sep.join([\"datasets\", dir, type, \"task\"]))[:-4]:\n",
    "            i = re.sub(r\"\\D\", \"\", i)\n",
    "            curr_MDP = MDP(dir, type, i)\n",
    "            if apply_to_grid(curr_MDP, False)[-1]:\n",
    "                sucesses += 1\n",
    "\n",
    "print(\"tasks solved with only imitation learning:\")\n",
    "print(f\"Total : {sucesses}, accuracy: {sucesses / len(valDataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is clearly seen here, imitation learning with the limited amount of training data is not able to solve most of the provided tasks. Therefore, in the following I will use the described PPO approach and after that will create the final model that uses imitation learning to initiate the model and then trains it with maskless PPO to achieve the best possible performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines with the PPO and Maskable PPO model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generated MDPs\n",
    "test = np.random.randint(1, 1e5)\n",
    "\n",
    "genMDP = MDP(\"generated\", \"train\", str(test))\n",
    "\n",
    "genMDP.print_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom environment\n",
    "from gym import spaces\n",
    "\n",
    "class Gridworld(gym.Env):\n",
    "\n",
    "    metadata = {\"render.modes\" : [\"human\"]}\n",
    "\n",
    "    def MDP_generator(self):\n",
    "        while True:\n",
    "            for dir in self.dir:\n",
    "                for type in self.type:\n",
    "                    for i in os.listdir(os.sep.join([\"datasets\", dir, type, \"task\"]))[:-4]:\n",
    "                        i = re.sub(r\"\\D\", \"\", i)\n",
    "                        yield dir, type, i\n",
    "\n",
    "    def __init__(self, dir = [\"data\", \"data_easy\", \"data_medium\"], type = [\"train\"]) -> None:\n",
    "        super(Gridworld, self).__init__()\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.observation_space = spaces.Box(low = 0, high = 10, shape = (2, 4, 4))\n",
    "\n",
    "        #available MDPs\n",
    "        self.dir = dir\n",
    "        self.type = type\n",
    "\n",
    "        self.next_MDP = self.MDP_generator()\n",
    "        self.actions = [\"move\", \"turnLeft\", \"turnRight\", \"pickMarker\", \"putMarker\", \"finish\"]\n",
    "\n",
    "    def reset(self):\n",
    "        nextDir, nextType, nexti =  next(self.next_MDP)\n",
    "        self.currentMDP = MDP(nextDir, nextType, nexti, lambda1 = 0.05)\n",
    "        return self.currentMDP.get_current_state() / 10\n",
    "\n",
    "    def step(self, action):\n",
    "        nextState, reward, done, info = self.currentMDP.sample_next_state_and_reward(self.actions[action])\n",
    "        return nextState / 10, reward, done, info\n",
    "        \n",
    "    def render(self):\n",
    "        self.currentMDP.print_grid()\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def action_masks(self):\n",
    "        return self.currentMDP.action_mask()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Gridworld()\n",
    "for _ in range(10):\n",
    "    env.reset()\n",
    "env.render()\n",
    "for a in [3,1,0,1,5]:\n",
    "    s, r, d, _ = env.step(a)\n",
    "    print(r, env.action_masks())\n",
    "    if d:\n",
    "        print(\"Finished\")\n",
    "    else:\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "env = make_vec_env(Gridworld)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose= 1)\n",
    "model.learn(total_timesteps= 1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RL_models(model):\n",
    "    valDataset = Gridworld(type = [\"val\"])\n",
    "    correct, total = 0,1e4\n",
    "    for task in range(int(total)):\n",
    "        if task % 5000 == 4999:\n",
    "            print(f\"{task+1*100 / total} %\")\n",
    "        currMDP = valDataset.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = model.predict(currMDP)[0]\n",
    "            if action == 5 and np.array_equal(currMDP[0], currMDP[1]): \n",
    "                correct += 1\n",
    "            currMDP, rew, done, _ = valDataset.step(action)\n",
    "            \n",
    "            \n",
    "    print(f\"correct : {correct}, accuracy: {correct*100/total } %\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RL_models(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure PPO without any accelaration techniques except the reward function did not solve a single task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maskable PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEnv = Gridworld()\n",
    "\n",
    "testEnv.reset()\n",
    "testEnv.reset()\n",
    "\n",
    "testEnv.render()\n",
    "print(testEnv.action_masks())\n",
    "\n",
    "testEnv.step(1)\n",
    "testEnv.step(0)\n",
    "testEnv.render()\n",
    "print(testEnv.action_masks())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskEnv = make_vec_env(Gridworld)\n",
    "\n",
    "\n",
    "maskModel = MaskablePPO(MaskableActorCriticPolicy, maskEnv, verbose= 1)\n",
    "maskModel.learn(5 * 1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = testEnv.reset()\n",
    "testEnv.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = maskModel.predict(test, action_masks = testEnv.action_masks(), deterministic= True)[0]\n",
    "print(action)\n",
    "test, rew, done, _ = testEnv.step(action)\n",
    "print(rew, done)\n",
    "testEnv.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RL_models(maskModel)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
