{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import torch \n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from MDP import MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3080'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['v' '.' '.' '#']\n",
      "  ['.' '#' '#' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['#' '#' '#' '#']]\n",
      "\n",
      " [['.' '.' '.' '#']\n",
      "  ['O' '#' '#' '#']\n",
      "  ['v' '#' '.' '#']\n",
      "  ['#' '#' '#' '#']]]\n",
      "[[['.' '.' '.' '#']\n",
      "  ['<' '#' '#' '#']\n",
      "  ['.' '#' '.' '#']\n",
      "  ['#' '#' '#' '#']]\n",
      "\n",
      " [['.' '.' '.' '#']\n",
      "  ['O' '#' '#' '#']\n",
      "  ['v' '#' '.' '#']\n",
      "  ['#' '#' '#' '#']]]\n"
     ]
    }
   ],
   "source": [
    "#load MDP\n",
    "mdp = MDP(dir = \"data_medium\", type = \"val\", name = \"100595\")\n",
    "# show grid\n",
    "mdp.print_grid()\n",
    "for a in [\"move\", \"turnRight\", \"pickMarker\", \"move\"]:\n",
    "    mdp.get_next_state(a)\n",
    "mdp.print_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model with Imitation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seq', 'task']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data tests\n",
    "os.listdir(\"datasets/data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "\n",
    "#numerical representation of actions\n",
    "getNumAction = {\n",
    "    \"move\" : 0,\n",
    "    \"turnRight\": 1,\n",
    "    \"turnLeft\" : 2,\n",
    "    \"pickMarker\": 3,\n",
    "    \"putMarker\" : 4,\n",
    "    \"finish\" : 5\n",
    "}\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    attributes:\n",
    "    dir : str list :=  accepted directories (data, data_easy, data_medium)\n",
    "    type : str list := train and/or val\n",
    "    grid : tensor := a tensor of all available grids\n",
    "    actions : tensor := vector of the optimal action for each\n",
    "    \"\"\"\n",
    "    def data_generator(self):\n",
    "        for dir in self.dir:\n",
    "            for type in self.type:\n",
    "                for i in os.listdir(os.sep.join([\"datasets\", dir, type, \"task\"]))[:-4]:\n",
    "                    i = re.sub(r\"\\D\", \"\", i)\n",
    "                    # load MDP and optimal sequence\n",
    "                    currMDP = MDP(dir = dir, type = type, name = str(i))\n",
    "\n",
    "                    with open(os.sep.join([\"datasets\", dir, type, \"seq\", str(i) + \"_seq.json\"])) as seq:\n",
    "                        sequence = json.load(seq)[\"sequence\"]\n",
    "                    \n",
    "                    for action in sequence:\n",
    "                        yield currMDP.get_current_state().copy(), action\n",
    "                        currMDP.get_next_state(action)\n",
    "\n",
    "\n",
    "    def __init__(self, dir = [\"data\", \"data_easy\", \"data_medium\"], type = [\"train\"]) -> None:\n",
    "        \"\"\"\n",
    "        dir : str list :=  accepted directories (data, data_easy, data_medium)\n",
    "        type : str list := train and/or val\n",
    "        \"\"\"\n",
    "        self.dir = dir\n",
    "        self.type = type\n",
    "        lstActionsAndGrids = list(self.data_generator())\n",
    "        self.grid = torch.tensor(np.array([x[0] for x in lstActionsAndGrids]) / 10)\n",
    "        self.actions = torch.tensor(np.array([getNumAction[x[1]] for x in lstActionsAndGrids]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.grid)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.grid[idx], self.actions[idx]\n",
    "\n",
    "trainDataset = Dataset()\n",
    "valDataset = Dataset(type = [\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 1.0000],\n",
       "         [1.0000, 0.0000, 1.0000, 0.0000],\n",
       "         [0.0000, 0.4000, 0.9000, 0.0000],\n",
       "         [0.0000, 0.0000, 1.0000, 0.0000]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 1.0000],\n",
       "         [1.0000, 0.4000, 1.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.9000, 0.0000],\n",
       "         [0.0000, 0.0000, 1.0000, 0.0000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Neural Network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    input : 2 X 4 X 4 grid\n",
    "    label : Move [0;6]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # first layer: input\n",
    "        self.conv1 = nn.Conv2d(2, 8, 2)\n",
    "\n",
    "        #second layer : 2nd convolution\n",
    "        self.conv2 = nn.Conv2d(8, 16, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 32, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(32, 64)\n",
    "\n",
    "        self.out = nn.Linear(64, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.out(x)\n",
    "        #x = F.softmax(x, dim = 0)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(2, 8, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=6, bias=True)\n",
      ")\n",
      "number of parameters: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating model\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(f\"number of parameters: {len(params)}\")\n",
    "\n",
    "#loss function\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "dataloader = data.DataLoader(trainDataset, BATCH_SIZE)\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        lossVal = loss(outputs, labels)\n",
    "        lossVal.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += lossVal.item()\n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "  batch 1000 loss: 1.5727730317115785\n",
      "  batch 2000 loss: 1.3680681443214417\n",
      "  batch 3000 loss: 1.3102844228744508\n",
      "  batch 4000 loss: 1.1738843717575074\n",
      "  batch 5000 loss: 1.2002523643970489\n",
      "EPOCH 2\n",
      "  batch 1000 loss: 1.1881384137868882\n",
      "  batch 2000 loss: 1.1391257843971252\n",
      "  batch 3000 loss: 1.0889491180181503\n",
      "  batch 4000 loss: 0.9354054371416569\n",
      "  batch 5000 loss: 0.9163639705181121\n",
      "EPOCH 3\n",
      "  batch 1000 loss: 0.9582524493336677\n",
      "  batch 2000 loss: 0.9409482676386833\n",
      "  batch 3000 loss: 0.898476741194725\n",
      "  batch 4000 loss: 0.8154534488022327\n",
      "  batch 5000 loss: 0.737059116512537\n",
      "EPOCH 4\n",
      "  batch 1000 loss: 0.8632445269525051\n",
      "  batch 2000 loss: 0.8447897163331508\n",
      "  batch 3000 loss: 0.8374769319295883\n",
      "  batch 4000 loss: 0.709097480431199\n",
      "  batch 5000 loss: 0.6351571732163429\n",
      "EPOCH 5\n",
      "  batch 1000 loss: 0.8166584567129612\n",
      "  batch 2000 loss: 0.7976168520152569\n",
      "  batch 3000 loss: 0.7644486688673496\n",
      "  batch 4000 loss: 0.6564892259538173\n",
      "  batch 5000 loss: 0.5683356807678938\n",
      "EPOCH 6\n",
      "  batch 1000 loss: 0.8005870394408703\n",
      "  batch 2000 loss: 0.7345171111524105\n",
      "  batch 3000 loss: 0.721756618976593\n",
      "  batch 4000 loss: 0.5986652463898062\n",
      "  batch 5000 loss: 0.5102617489024996\n",
      "EPOCH 7\n",
      "  batch 1000 loss: 0.7718633663654327\n",
      "  batch 2000 loss: 0.7012658788859845\n",
      "  batch 3000 loss: 0.6868004705905915\n",
      "  batch 4000 loss: 0.6115340729206801\n",
      "  batch 5000 loss: 0.4699646003395319\n",
      "EPOCH 8\n",
      "  batch 1000 loss: 0.712534400433302\n",
      "  batch 2000 loss: 0.6746327101290226\n",
      "  batch 3000 loss: 0.6615700645744801\n",
      "  batch 4000 loss: 0.5344670154862106\n",
      "  batch 5000 loss: 0.47824436946213245\n",
      "EPOCH 9\n",
      "  batch 1000 loss: 0.6688401145040989\n",
      "  batch 2000 loss: 0.6464774076938629\n",
      "  batch 3000 loss: 0.6381447158455849\n",
      "  batch 4000 loss: 0.510877772230655\n",
      "  batch 5000 loss: 0.4365602683052421\n",
      "EPOCH 10\n",
      "  batch 1000 loss: 0.6533002208769322\n",
      "  batch 2000 loss: 0.6349763856232167\n",
      "  batch 3000 loss: 0.6479171352386475\n",
      "  batch 4000 loss: 0.4914506800677627\n",
      "  batch 5000 loss: 0.39949084095470605\n",
      "EPOCH 11\n",
      "  batch 1000 loss: 0.6241717448532581\n",
      "  batch 2000 loss: 0.626390535801649\n",
      "  batch 3000 loss: 0.58898911100626\n",
      "  batch 4000 loss: 0.4957031854391098\n",
      "  batch 5000 loss: 0.4643757348693907\n",
      "EPOCH 12\n",
      "  batch 1000 loss: 0.6074572918862104\n",
      "  batch 2000 loss: 0.6015761087536812\n",
      "  batch 3000 loss: 0.5761606488227844\n",
      "  batch 4000 loss: 0.48429446640238166\n",
      "  batch 5000 loss: 0.3722936707660556\n",
      "EPOCH 13\n",
      "  batch 1000 loss: 0.610541819870472\n",
      "  batch 2000 loss: 0.5838033159524202\n",
      "  batch 3000 loss: 0.5674682058393955\n",
      "  batch 4000 loss: 0.456758484326303\n",
      "  batch 5000 loss: 0.3978490683790296\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "writer = SummaryWriter(\"runs/imitation_learning_{}\".format(timestamp))\n",
    "epoch_num = 0\n",
    "\n",
    "EPOCHS = 13\n",
    "\n",
    "best_vloss = 1000000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH {}\".format(epoch_num + 1))\n",
    "\n",
    "    net.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_num, writer)\n",
    "\n",
    "    net.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # TODO: implement validation\n",
    "\n",
    "\n",
    "    writer.add_scalars(\"Training Loss\", {\"Training\" : avg_loss})\n",
    "    writer.flush()\n",
    "\n",
    "    epoch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['#' '.' '.' '#']\n",
      "  ['u' 'O' '.' '#']\n",
      "  ['#' '.' '#' '.']\n",
      "  ['.' '.' '.' '#']]\n",
      "\n",
      " [['#' '.' '.' '#']\n",
      "  ['.' 'O' '>' '#']\n",
      "  ['#' '.' '#' '.']\n",
      "  ['.' '.' '.' '#']]]\n",
      "action: pickMarker, reward: 1.0\n",
      "[[['#' '.' '.' '#']\n",
      "  ['^' 'O' '.' '#']\n",
      "  ['#' '.' '#' '.']\n",
      "  ['.' '.' '.' '#']]\n",
      "\n",
      " [['#' '.' '.' '#']\n",
      "  ['.' 'O' '>' '#']\n",
      "  ['#' '.' '#' '.']\n",
      "  ['.' '.' '.' '#']]]\n",
      "action: move, reward: 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 2)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model\n",
    "actions = [\"move\", \"turnRight\", \"turnLeft\", \"pickMarker\", \"putMarker\", \"finish\"]\n",
    "\n",
    "testMDP = MDP(dir = \"data\", type = \"val\", name = \"101778\")\n",
    "\n",
    "def apply_to_grid(MDP):\n",
    "    reward = 0\n",
    "    steps = 0\n",
    "    MDP.print_grid()\n",
    "    while True:\n",
    "        current_grid = torch.tensor(MDP.get_current_state()) / 10\n",
    "        out = net(current_grid.unsqueeze(0))\n",
    "        nextAction = actions[torch.argmax(out)]\n",
    "        reward = MDP.gamma * reward + MDP.reward(nextAction)\n",
    "        steps += 1\n",
    "        print(\"action: {}, reward: {}\".format(nextAction, reward))\n",
    "        if MDP.get_next_state(nextAction) == \"Terminal\":\n",
    "            return reward, steps\n",
    "        MDP.print_grid()\n",
    "\n",
    "\n",
    "apply_to_grid(testMDP)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
